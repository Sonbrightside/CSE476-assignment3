{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPiYnCqO96sl"
      },
      "source": [
        "#Assignment 3: Decoder-only Transformer\n",
        "\n",
        "Hello! Welcome to your last assignment, where you will be asked to implement a decoder-only transformer model, and train it on next word prediction objectives.\n",
        "\n",
        "Because this is a GPU-based assignment, you are encouraged to use Google Colab's T4 runtime. It contains a 15G GPU, which we have tested to be enough for this assignment. However, there is a limited amount of free T4 usage per day. We encourage you to only connect to a runtime when you want to run the code.\n",
        "\n",
        "You are free to make minor changes to the provided code if that will make things eaiser. However, you will be asked to summarize what you changed in the final section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ruQRT7ZvdvQU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy-2.2.0-py3.10-macosx-10.9-universal2.egg (from datasets) (2.2.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (19.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas-2.2.3-py3.10-macosx-10.9-universal2.egg (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm-4.67.1-py3.10.egg (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.28.0)\n",
            "Requirement already satisfied: packaging in /Users/hongjuson/Library/Python/3.10/lib/python/site-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/hongjuson/Library/Python/3.10/lib/python/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/hongjuson/Library/Python/3.10/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytz-2024.2-py3.10.egg (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tzdata-2024.2-py3.10.egg (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/hongjuson/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWh0E9ZG-aDS"
      },
      "source": [
        "First, we will define some hyperparameters. You do not need to change these hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B7dKR9nubIs0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = 32100\n",
        "embedding_dim = 256\n",
        "num_heads = 8\n",
        "num_layers = 8\n",
        "ffn_dim = 256\n",
        "max_seq_len = 256\n",
        "batch_size = 64\n",
        "num_epochs = 1\n",
        "learning_rate = 0.001\n",
        "unk_token = \"<UNK>\"\n",
        "pad_token = \"<PAD>\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_K43jK3-hd7"
      },
      "source": [
        "## Section 1: Implement the Decoder-only Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FYuZTvzJcg81"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Tokenizer\n",
        "# We will use a pre-trained tokenizer from T5-base, which is why vocab_size is 32100 above.\n",
        "# You will need to research on how huggingface tokenizers work\n",
        "# https://huggingface.co/docs/transformers/en/main_classes/tokenizer\n",
        "class BPETokenizer:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        # TODO\n",
        "        # Implement this encode function that return a list of INTs (word IDs) based on the input text (str)\n",
        "        # Hint: you can use the tokenizer.encode function, but you need to add the special tokens.\n",
        "        return self.tokenizer.encode(text, add_special_tokens=True)\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        # TODO\n",
        "        # Implement this decode function that returns a string based on a list of INTs (word IDs)\n",
        "        return self.tokenizer.decode(token_ids, skip_special_tokens=True)\n",
        "# Positional Encoding\n",
        "# TODO: You need to implement a sinusoidal positional embedding class.\n",
        "# There are many great resources, such as https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\n",
        "# Input: current word embedding\n",
        "# Output: new word embedding after adding the positional embeddings.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embedding_dim, max_len):\n",
        "        super().__init__()\n",
        "        # TODO\n",
        "        wording_embedding = torch.zeros(max_len, embedding_dim)\n",
        "        for pos in range(max_len):\n",
        "            for i in range(0, embedding_dim, 2):\n",
        "                wording_embedding[pos, i] = torch.sin(pos / (10000 ** ((2 * i) / embedding_dim)))\n",
        "                if i + 1 < embedding_dim:\n",
        "                    wording_embedding[pos, i + 1] = torch.cos(pos / (10000 ** ((2 * (i + 1)) / embedding_dim)))\n",
        "                \n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO\n",
        "        # Implement the forward function that adds the positional embeddings to the input word embeddings\n",
        "        # Hint: you can use x + self.wording_embedding[:x.size(1), :], but you need to make sure that the dimensions match.\n",
        "        # You can use x.size(1) to get the length of the input sequence.\n",
        "        # Hint: you can use x.size(0) to get the batch size.\n",
        "        # Hint: you can use x.size(2) to get the embedding dimension.\n",
        "        # Hint: you can use x.size(1) to get the length of the input sequence.\n",
        "        for i in range(x.size(0)):\n",
        "            x[i] = x[i] + self.wording_embedding[:x.size(1), :] \n",
        "        return x\n",
        "        \n",
        "# Self-Attention Layer\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embedding_dim % num_heads == 0\n",
        "        # TODO\n",
        "        # Hint: you need the QKV weights, also some bookkeeping values.\n",
        "        # Don't forget about a fully-connected output layer\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.attention_dim = embedding_dim // num_heads\n",
        "        \n",
        "        self.query = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.key = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.value = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "        self.fc_out = nn.Linear(embedding_dim, embedding_dim)\n",
        "        \n",
        "\n",
        "\n",
        "    # Attention masks are values of 1's and 0's.\n",
        "    # When the attention mask for a position is 1, it means that the attention can be computed on that position.\n",
        "    # If a mask value for a position is 0, it means that the attention should 'skip' or 'mask-out' that position.\n",
        "    def forward(self, x, mask=None):\n",
        "        # TODO\n",
        "        # Implement the computations of attention scores, with respect to the attentin mask 'mask'\n",
        "        # Hint: consider something like attention_scores.masked_fill(?, ?) to compute the masked scores.\n",
        "        query = self.qury(x)\n",
        "        key = self.key(x)\n",
        "        value = self.value(x)\n",
        "\n",
        "        # TODO\n",
        "        # Implement the scaled dot-product attention\n",
        "        # Hint: you can use torch.matmul to compute the dot product.\n",
        "        # Hint: you can use torch.softmax to compute the attention scores.\n",
        "        \n",
        "        query = query.view(batch_size, max_seq_len, self.num_heads, self.attention_dim).transpose(1, 2)\n",
        "        key = key.view(batch_size, max_seq_len, self.num_heads, self.attention_dim).transpose(1, 2)\n",
        "        value = value.view(batch_size, max_seq_len, self.num_heads, self.attention_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(Q.size(-1))\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attention_scores = F.softmax(scores, dim=-1)\n",
        "        attention_output = torch.matmul(attention_scores, V)\n",
        "        \n",
        "\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
        "        concat_attention = attention_output.view(batch_size, max_seq_len, self.embedding_dim)\n",
        "\n",
        "        output = self.fc_out(concat_attention)\n",
        "\n",
        "        return output\n",
        "  \n",
        "\n",
        "# Transformer Decoder Layer\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, ffn_dim):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadSelfAttention(embedding_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
        "        # TODO\n",
        "        # Implement a MLP layer here.\n",
        "        # Research about what nn.LayerNorm is doing here.\n",
        "        # Add a second layernorm after the MLP.\n",
        "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, ffn_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ffn_dim, embedding_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.self_attn(x, mask)\n",
        "        # TODO: research about why it is x + attn_output, instead of only attn_output?\n",
        "        x = self.norm1(x + attn_output)\n",
        "        # TODO: finish the rest.\n",
        "        x = self.norm2(x + self.ffn(x))\n",
        "\n",
        "# Decoder-only Transformer Model\n",
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, ffn_dim, max_seq_len):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_encoding = PositionalEncoding(embedding_dim, max_seq_len)\n",
        "        # TODO\n",
        "        # Implement the layers\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerDecoderLayer(embedding_dim, num_heads, ffn_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.ln_final = nn.LayerNorm(embedding_dim)\n",
        "        self.fc_out = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoding(x)\n",
        "        # TODO\n",
        "        # Create an attention mask, and do the rest of the computation.\n",
        "        seq_len = x.size(1)\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x, mask)\n",
        "        x = self.ln_final(x)\n",
        "        \n",
        "        x = self.fc_out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASD3chyEHVwv"
      },
      "source": [
        "## Section 2: Implement the LLM training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vT6_x9ShcoSR"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (736481251.py, line 53)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 53\u001b[0;36m\u001b[0m\n\u001b[0;31m    inputs = # TODO\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "def load_training_corpus():\n",
        "    dataset = load_dataset(\"alpindale/light-novels\", split=\"train\")\n",
        "    texts = dataset[\"text\"][:3000000]\n",
        "    tokenizer = BPETokenizer()\n",
        "    tokenized = []\n",
        "    # TODO\n",
        "    # In pre-training, we will make every training sequence with exact same lengths (max_seq_len)\n",
        "    # This means that you will to concatenate shorter text in 'texts' to make every list in 'tokenized' exactly max_seq_len\n",
        "    # In other words, for instance in tokenized: assert len(instance) == max_seq_len.\n",
        "    # As you can see later, each element in tokenized is a number (word IDs).\n",
        "    # For the final instance where you don't have enough tokens, you can pad the sequence with word id 1.\n",
        "    \n",
        "    for text in texts:\n",
        "        tokenized_text = tokenizer.encode(text)\n",
        "        if len(tokenized_text) < max_seq_len:\n",
        "            tokenized_text += [1] * (max_seq_len - len(tokenized_text))\n",
        "        else:\n",
        "            tokenized_text = tokenized_text[:max_seq_len]\n",
        "        tokenized.append(tokenized_text)\n",
        "\n",
        "    return torch.tensor(tokenized, dtype=torch.long), tokenizer\n",
        "\n",
        "data, tokenizer = load_book_corpus()\n",
        "train_data = data\n",
        "\n",
        "def get_lr_schedule(optimizer, warmup_steps, total_steps):\n",
        "    \"\"\"\n",
        "    Creates a learning rate schedule with linear warmup and cosine decay.\n",
        "    \"\"\"\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < warmup_steps:\n",
        "            return current_step / warmup_steps  # Linear warmup\n",
        "        progress = (current_step - warmup_steps) / (total_steps - warmup_steps)\n",
        "        return 0.5 * (1 + torch.cos(torch.tensor(progress * 3.1415926535)))  # Cosine decay\n",
        "\n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "def train():\n",
        "    model = DecoderOnlyTransformer(vocab_size, embedding_dim, num_heads, num_layers, ffn_dim, max_seq_len).to(device)\n",
        "    # TODO: compute the model size. You can use a programmatic approach.\n",
        "    \n",
        "    num_params = 0\n",
        "\n",
        "    for param in model.parameters():\n",
        "        num_params += param.numel()\n",
        "    \n",
        "    print(f\"Model size: {num_params} parameters\")\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    warmup_steps = int(0.1 * (num_epochs * len(train_data) // batch_size))  # 10% of total steps\n",
        "    total_steps = num_epochs * (len(train_data) // batch_size)\n",
        "    scheduler = get_lr_schedule(optimizer, warmup_steps, total_steps)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(range(0, len(train_data), batch_size), desc=f\"Epoch {epoch+1}\")\n",
        "        for i in progress_bar:\n",
        "            batch = train_data[i:i+batch_size].to(device)\n",
        "            # TODO Implement the training process.\n",
        "            inputs =  batch[:, :-1] # TODO\n",
        "            targets = batch[:,1:]# TODO. What is supposed to be the supervision targets in autogressive LM?\n",
        "\n",
        "            outputs = model(inputs)  \n",
        "\n",
        "            loss = loss_fn(outputs.view(-1, vocab_size), targets.reshape(-1))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Your code should finish before scheduler.step() here.\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "        print(f\"Epoch {epoch+1} completed with Loss: {total_loss / len(train_data)}\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0ScrjIVHOE4"
      },
      "source": [
        "Now, let's train the model! We suggest you save the trained model for later testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d2Vvg45HLwq"
      },
      "outputs": [],
      "source": [
        "model = train()\n",
        "torch.save(model.state_dict(), \"model.ckpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKidJSQZHUGy"
      },
      "source": [
        "Then, let's evaluate your model to see if it's working!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrYqG14H3Z4v"
      },
      "outputs": [],
      "source": [
        "# We have provided a sample greedy decoding generation function for you.\n",
        "# You can use it to test if your trained model is working.\n",
        "# Although we are not providing any actual test cases, it will be pretty easy\n",
        "# to tell if your language model is generating coherent natural languages.\n",
        "# Try a few examples! If it works, it works; we will not do 'exact-match' grading.\n",
        "model.eval()\n",
        "def generate_text_greedy(model, input_text, max_length=20):\n",
        "    input_tokens = tokenizer.encode(input_text)\n",
        "    generated = input_tokens[:]\n",
        "    for _ in range(max_length - len(input_tokens)):\n",
        "        input_tensor = torch.tensor([generated], device=device)\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)\n",
        "        next_token = torch.argmax(output[:, -1, :], dim=-1).item()\n",
        "        generated.append(next_token)\n",
        "        if next_token == 1:\n",
        "            break\n",
        "    return tokenizer.decode(generated)\n",
        "\n",
        "print(\"Generated Sequence:\", generate_text_greedy(model, \"what is your\"))\n",
        "\n",
        "def generate_text_topk(model, input_text, max_length=20, k=50, temperature=1.0):\n",
        "    # TODO\n",
        "    # Implement a top-k sampling generation function.\n",
        "    # Steps\n",
        "    # 1. adjust the output distribution/logits based on the temperature\n",
        "    # 2. find the top-k next tokens (hint: torch.topk)\n",
        "    # 3. readjust the probability for the top-k candidates for them to sum to 1 (hint: softmax)\n",
        "    # 4. sample a word from the new k tokens (hint: torch.multinomial)\n",
        "    input_tokens = tokenizer.encode(input_text)\n",
        "    generated = input_tokens[:]\n",
        "    for _ in range(max_length - length(input_tokens)):\n",
        "        input_tensor = torch.tensor([generated], device=device)\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)\n",
        "        logits = output[:, -1, :] / temperature\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        topk_probs, topk_indices = torch.topk(probs, k)\n",
        "        topk_probs = topk_probs / torch.sum(topk_probs)\n",
        "        next_token = torch.multinomial(topk_probs, 1).item()\n",
        "        generated.append(topk_indices[0][next_token].item())\n",
        "\n",
        "    return tokenizer.decode(generated)\n",
        "        \n",
        "\n",
        "print(\"Generated Sequence:\", generate_text_topk(model, \"what is your\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt7qiuDpJB6H"
      },
      "source": [
        "## Section 3: Written questions.\n",
        "Answer all questions with ~100 words (use the fewest words that are enough to express your meanings)\n",
        "\n",
        "### 1. What does nn.LayerNorm do?\n",
        "\n",
        "### 2. Why the provided code use self.norm1(x + attn_output) instead of self.norm1(attn_output)?\n",
        "\n",
        "### 3. What is the model size using the provided hyperparameters?\n",
        "\n",
        "### 4. How does temperature play a part in top-k sampling? Share some observations.\n",
        "\n",
        "### 5. (Optional, not graded) Please summarize your change to the provided code, if any.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
